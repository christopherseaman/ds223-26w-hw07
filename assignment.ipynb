{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944ffe31",
   "metadata": {},
   "source": [
    "# Assignment 7: Clinical NLP with LLMs and Embeddings\n",
    "\n",
    "Extract structured data from clinical notes using LLM prompt engineering, then build a semantic search system using sentence embeddings.\n",
    "\n",
    "**Dataset:** 75 synthetic discharge summaries from [Asclepius-Synthetic-Clinical-Notes](https://huggingface.co/datasets/aisc-team-a1/Asclepius-Synthetic-Clinical-Notes) (Kweon et al., 2023) in `asclepius_notes.json`.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt\n",
    "\n",
    "# Clear state after installing packages. If you re-run cells out of order later, re-run this cell first.\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "load_dotenv()\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b2f75",
   "metadata": {},
   "source": [
    "### API Key\n",
    "\n",
    "Part 1 requires an [OpenRouter](https://openrouter.ai) API key (OpenAI keys also work). Add the key from class forum to `.env` (not `example.env`). It should look like:\n",
    "\n",
    "```bash\n",
    "OPENROUTER_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "### Helper Functions (modify at your own risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf760aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM client setup (do not modify) ---\n",
    "\n",
    "def get_client():\n",
    "    \"\"\"Initialize the LLM client based on available API keys.\"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    if os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "        client = OpenAI(\n",
    "            api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "        )\n",
    "        return client, \"openrouter\"\n",
    "\n",
    "    if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        return OpenAI(), \"openai\"\n",
    "\n",
    "    raise ValueError(\n",
    "        \"No API key found. Set OPENROUTER_API_KEY or OPENAI_API_KEY in .env\"\n",
    "    )\n",
    "\n",
    "\n",
    "def call_llm(prompt, provider, client):\n",
    "    \"\"\"Send a prompt to the LLM and return the response text.\"\"\"\n",
    "    model = \"openai/gpt-4o-mini\" if provider == \"openrouter\" else \"gpt-4o-mini\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a medical information extraction assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Detect the best available device for local model inference.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c0f89",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"asclepius_notes.json\") as f:\n",
    "    asclepius = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(asclepius)} synthetic clinical notes\")\n",
    "print(f\"Keys: {list(asclepius[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4529e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asclepius[0][\"note\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5604dc8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Clinical Entity Extraction\n",
    "\n",
    "Use LLM prompt engineering to extract structured medical data from clinical notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e923cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 4 notes for extraction\n",
    "random.seed(2026)\n",
    "sample = random.sample(asclepius, 4)\n",
    "notes_p1 = [s[\"note\"] for s in sample]\n",
    "\n",
    "print(f\"Selected {len(notes_p1)} notes for extraction\")\n",
    "for i, n in enumerate(notes_p1, 1):\n",
    "    print(f\"\\n--- Note {i} ({len(n)} chars) ---\")\n",
    "    print(n[:150] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5907063",
   "metadata": {},
   "source": [
    "### `build_prompt`\n",
    "\n",
    "Build a prompt that instructs the LLM to extract structured data from a clinical note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement build_prompt\n",
    "# Requirements:\n",
    "#   - Describe the extraction task clearly\n",
    "#   - Specify the JSON output schema with these fields:\n",
    "#     {\"diagnosis\": str, \"medications\": list, \"lab_values\": dict, \"confidence\": float}\n",
    "#   - When few_shot=True, include 1-2 example input/output pairs\n",
    "#   - Include the clinical note text\n",
    "def build_prompt(note, few_shot=False):\n",
    "    pass  # replace with your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67c11b",
   "metadata": {},
   "source": [
    "### `parse_json_response`\n",
    "\n",
    "Extract a JSON object from LLM response text, which may contain markdown code fences or other wrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38847c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement parse_json_response\n",
    "# Requirements:\n",
    "#   - Handle clean JSON strings (direct json.loads)\n",
    "#   - Handle JSON wrapped in ```json ... ``` markdown blocks\n",
    "#   - Find JSON within surrounding text (look for outermost { and })\n",
    "#   - Return None if parsing fails\n",
    "def parse_json_response(text):\n",
    "    pass  # replace with your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12163e8",
   "metadata": {},
   "source": [
    "### `validate_response`\n",
    "\n",
    "Check that a parsed response dict contains all required keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement validate_response\n",
    "# Required fields: diagnosis, medications, lab_values, confidence\n",
    "# Return True if all present, False otherwise\n",
    "def validate_response(response):\n",
    "    pass  # replace with your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1762c",
   "metadata": {},
   "source": [
    "### `extract_entities`\n",
    "\n",
    "Orchestrate the full extraction pipeline: get client, build prompt, call LLM, parse, validate, return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87156a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement extract_entities\n",
    "# Steps:\n",
    "#   1. client, provider = get_client()\n",
    "#   2. prompt = build_prompt(note, few_shot=few_shot)\n",
    "#   3. raw = call_llm(prompt, provider=provider, client=client)\n",
    "#   4. parsed = parse_json_response(raw)\n",
    "#   5. Validate and return (return None if parsing or validation fails)\n",
    "def extract_entities(note, few_shot=False):\n",
    "    pass  # replace with your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0f62e",
   "metadata": {},
   "source": [
    "### Test extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488e998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p1 = []\n",
    "for i, note in enumerate(notes_p1, 1):\n",
    "    result = extract_entities(note, few_shot=True)\n",
    "    print(f\"--- Note {i} ---\")\n",
    "    if result:\n",
    "        print(json.dumps(result, indent=2))\n",
    "        results_p1.append(result)\n",
    "    else:\n",
    "        print(\"Extraction failed\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5957f3",
   "metadata": {},
   "source": [
    "### Save Part 1 results (do not modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10e4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/extraction_results.json\", \"w\") as f:\n",
    "    json.dump(results_p1, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(results_p1)} extraction results to output/extraction_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717dd34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Semantic Search\n",
    "\n",
    "Build a semantic search system that finds clinical notes by meaning rather than keywords, using sentence embeddings and cosine similarity.\n",
    "\n",
    "This part runs locally — no API key needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=get_device())\n",
    "print(f\"Model loaded on {get_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all 75 notes for the search corpus\n",
    "notes_p2 = [n[\"note\"] for n in asclepius]\n",
    "print(f\"{len(notes_p2)} notes in search corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda13c04",
   "metadata": {},
   "source": [
    "### `embed_notes`\n",
    "\n",
    "Generate embeddings for a list of notes using the sentence transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2804b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement embed_notes\n",
    "# Use model.encode(notes) — returns a numpy array of shape (n_notes, embedding_dim)\n",
    "def embed_notes(notes):\n",
    "    pass  # replace with your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa3f61",
   "metadata": {},
   "source": [
    "### `find_similar`\n",
    "\n",
    "Search notes by meaning using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab447e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement find_similar\n",
    "# Steps:\n",
    "#   1. Embed the query with model.encode([query])\n",
    "#   2. Compute cosine_similarity(query_embedding, embeddings)\n",
    "#   3. Sort by score descending\n",
    "#   4. Return top_k results as [{\"note\": str, \"score\": float}, ...]\n",
    "#\n",
    "# Note: this function uses the `model` variable from notebook scope.\n",
    "# This is a common notebook pattern — the model is loaded once and reused\n",
    "# across cells. Outside a notebook you'd pass the model as a parameter.\n",
    "def find_similar(query, notes, embeddings, top_k=2):\n",
    "    pass  # replace with your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dee702",
   "metadata": {},
   "source": [
    "### Run the search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df63a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed_notes(notes_p2)\n",
    "print(f\"Embeddings: {embeddings.shape}\")\n",
    "\n",
    "queries = [\n",
    "    \"heart attack symptoms\",\n",
    "    \"infectious disease with fever\",\n",
    "    \"respiratory illness\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQuery: '{q}'\")\n",
    "    results = find_similar(q, notes_p2, embeddings, top_k=2)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  {i}. (score: {r['score']:.3f}) {r['note'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c261a4",
   "metadata": {},
   "source": [
    "### Save Part 2 results (do not modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = find_similar(\"heart attack symptoms\", notes_p2, embeddings, top_k=3)\n",
    "with open(\"output/search_results.json\", \"w\") as f:\n",
    "    json.dump(search_results, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(search_results)} search results to output/search_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c169b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2789bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run 'python -m pytest .github/tests/ -v' in your terminal to check your work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9591c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Build a Tiny LLM *(optional, not graded)*\n",
    "\n",
    "Train a character-level transformer to generate new text from a dataset of short strings. This mirrors the microGPT demo from lecture — same architecture, different data, using PyTorch's built-in modules instead of writing everything from scratch.\n",
    "\n",
    "**Choose your dataset** (or use both!):\n",
    "\n",
    "| Dataset | File | Items | Description |\n",
    "|:---|:---|:---|:---|\n",
    "| D&D Spells | `dnd_spells.lst` | 518 | Official spell names from Dungeons & Dragons |\n",
    "| Ice Cream | `icecream_flavors.lst` | 450 | Ice cream flavor names from a [CMU student survey](https://www.cs.cmu.edu/~15110-f23/slides/all-icecream.csv) |\n",
    "\n",
    "The code below uses D&D spells — swap the filename and variable names if you prefer ice cream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06dc1f",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1605f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your dataset: \"dnd_spells.lst\" or \"icecream_flavors.lst\"\n",
    "datafile = \"dnd_spells.lst\"\n",
    "\n",
    "with open(datafile) as f:\n",
    "    lines = f.read().strip().split(\"\\n\")\n",
    "items = [line.strip() for line in lines[1:] if line.strip()]  # skip header\n",
    "\n",
    "text = \"\\n\".join(items)\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Character <-> integer mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"{len(items)} items from {datafile}\")\n",
    "print(f\"{len(chars)} unique characters, {len(data)} total tokens\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c799737",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "This is a minimal GPT: token embeddings + position embeddings → transformer decoder → output head. Read through the code, then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32   # context window (characters)\n",
    "n_embd = 64       # embedding dimension\n",
    "n_head = 4        # attention heads\n",
    "n_layer = 2       # transformer blocks\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "class CharGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each character gets a learnable vector of size n_embd\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        # Each position (0..block_size-1) also gets a learnable vector\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Stack of transformer decoder layers — this is where attention happens\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=n_embd,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=4 * n_embd,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=n_layer)\n",
    "\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        # Project from embedding space back to vocabulary size (one logit per character)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok = self.tok_emb(idx)                                    # (B, T, n_embd)\n",
    "        pos = self.pos_emb(torch.arange(T, device=idx.device))    # (T, n_embd)\n",
    "        x = self.drop(tok + pos)                                   # (B, T, n_embd)\n",
    "\n",
    "        # Causal mask: prevents each position from attending to future positions\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(T, device=idx.device)\n",
    "        x = self.transformer(x, x, tgt_mask=mask, memory_mask=mask)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "char_model = CharGPT().to(device)\n",
    "print(f\"CharGPT: {sum(p.numel() for p in char_model.parameters()):,} parameters on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ca424",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "The training loop samples random chunks from the data and teaches the model to predict the next character. Loss should drop below ~2.0 after 2000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5425b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(char_model.parameters(), lr=3e-4)\n",
    "batch_size = 32\n",
    "steps = 2000\n",
    "\n",
    "for step in range(steps):\n",
    "    # Pick random starting positions\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix]).to(device)\n",
    "\n",
    "    logits, loss = char_model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0 or step == steps - 1:\n",
    "        print(f\"step {step:4d} | loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c249d6",
   "metadata": {},
   "source": [
    "### Generate\n",
    "\n",
    "Sample from the trained model at different temperatures. Lower temperature = more conservative (common patterns), higher = more creative (weirder output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9419e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, max_new_tokens=500, temperature=0.8):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([[stoi[\"\\n\"]]], device=device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = idx[:, -block_size:]\n",
    "        logits, _ = model(context)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "    model.train()\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "\n",
    "for temp in [0.5, 0.8, 1.2]:\n",
    "    print(f\"\\n--- Temperature {temp} ---\")\n",
    "    output = generate(char_model, temperature=temp)\n",
    "    names = [s.strip() for s in output.split(\"\\n\") if s.strip()]\n",
    "    for name in names[:10]:\n",
    "        print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001e7e5",
   "metadata": {},
   "source": [
    "### Experiment (optional)\n",
    "\n",
    "Try changing things and see what happens:\n",
    "\n",
    "- Switch datasets — do ice cream flavors vs spell names produce different quality output?\n",
    "- Increase `n_layer` to 4 or `n_embd` to 128 — does the model improve? How much slower is training?\n",
    "- Train for 5000 steps instead of 2000\n",
    "- What happens at very low temperature (0.2) vs very high (2.0)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
